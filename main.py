import streamlit as st
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from joblib import Parallel, delayed
import warnings
warnings.filterwarnings("ignore")

# ===========================
# âš™ï¸ Cáº¤U HÃŒNH GIAO DIá»†N
# ===========================
st.set_page_config(page_title="ğŸ² AI Dá»± Ä‘oÃ¡n TÃ i Xá»‰u NÃ¢ng Cao", layout="wide")
st.title("ğŸ² AI Dá»± Ä‘oÃ¡n TÃ i Xá»‰u NÃ¢ng Cao")
st.markdown("#### ğŸ¤– Huáº¥n luyá»‡n thÃ´ng minh + 2 cháº¿ Ä‘á»™ Ensemble: Weighted Voting & Stacking Meta-Learning")
st.divider()

# ===========================
# ğŸ§© THIáº¾T Láº¬P
# ===========================
MAX_TRAIN_SAMPLES = 3000
TEST_SIZE = 0.2
SEED = 42

# ===========================
# ğŸ“Š Táº O Dá»® LIá»†U GIáº¢
# ===========================
@st.cache_data
def create_data(n_samples=MAX_TRAIN_SAMPLES):
    X = np.random.randn(n_samples, 8)
    y = np.random.randint(0, 2, size=n_samples)
    return X, y

X, y = create_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)

# ===========================
# ğŸ§  KHá»I Táº O MÃ” HÃŒNH
# ===========================
def get_models():
    return {
        "XGBoost": XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=-1, verbosity=0, random_state=SEED),
        "CatBoost": CatBoostClassifier(iterations=100, depth=3, learning_rate=0.1, verbose=0, random_state=SEED),
        "RandomForest": RandomForestClassifier(n_estimators=100, max_depth=5, n_jobs=-1, random_state=SEED),
        "Logistic": LogisticRegression(max_iter=500, solver="lbfgs", random_state=SEED)
    }

# ===========================
# âš¡ HUáº¤N LUYá»†N SONG SONG
# ===========================
@st.cache_resource
def train_all(models, X_train, y_train):
    def train_one(name, model):
        model.fit(X_train, y_train)
        return name, model
    results = Parallel(n_jobs=4)(delayed(train_one)(n, m) for n, m in models.items())
    return dict(results)

# ===========================
# ğŸ§® WEIGHTED VOTING
# ===========================
def weighted_voting(models, X, weights=None):
    base_probs = {}
    for name, model in models.items():
        base_probs[name] = model.predict_proba(X)[:, 1]
    keys = list(base_probs.keys())
    if weights is None:
        weights = {k: 1.0 / len(keys) for k in keys}
    combined = np.zeros_like(base_probs[keys[0]])
    for k in keys:
        combined += base_probs[k] * weights[k]
    combined /= sum(weights.values())
    return combined

# ===========================
# ğŸ§  STACKING META
# ===========================
def stacking_meta(models, X_train, y_train, X_test, folds=3):
    kf = KFold(n_splits=folds, shuffle=True, random_state=SEED)
    meta_train = np.zeros((len(X_train), len(models)))
    meta_test = np.zeros((len(X_test), len(models)))

    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr, y_val = y_train[train_idx], y_train[val_idx]
        for i, (name, model) in enumerate(models.items()):
            model.fit(X_tr, y_tr)
            meta_train[val_idx, i] = model.predict_proba(X_val)[:, 1]
            meta_test[:, i] += model.predict_proba(X_test)[:, 1] / folds

    meta_model = LogisticRegression(max_iter=500)
    meta_model.fit(meta_train, y_train)
    return meta_model, meta_model.predict_proba(meta_test)[:, 1]

# ===========================
# ğŸ–¥ï¸ GIAO DIá»†N
# ===========================
if "trained_models" not in st.session_state:
    st.session_state.trained_models = None
    st.session_state.results = None

col1, col2 = st.columns(2)
with col1:
    if st.button("ğŸš€ Huáº¥n luyá»‡n MÃ´ HÃ¬nh"):
        with st.spinner("ğŸ”„ Äang huáº¥n luyá»‡n 4 mÃ´ hÃ¬nh song song..."):
            models = get_models()
            trained_models = train_all(models, X_train, y_train)

            results = {}
            for name, model in trained_models.items():
                preds = model.predict(X_test)
                acc = accuracy_score(y_test, preds)
                results[name] = round(acc * 100, 2)

            st.session_state.trained_models = trained_models
            st.session_state.results = results
            st.success("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t! MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong bá»™ nhá»›.")

with col2:
    if st.button("ğŸ§® Dá»± Ä‘oÃ¡n Ensemble (Voting + Stacking)"):
        if st.session_state.trained_models is None:
            st.warning("âš ï¸ Báº¡n cáº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh trÆ°á»›c.")
        else:
            models = st.session_state.trained_models
            results = st.session_state.results

            weights = {k: v / sum(results.values()) for k, v in results.items()}
            vote_probs = weighted_voting(models, X_test, weights)
            vote_preds = (vote_probs > 0.5).astype(int)
            acc_vote = round(accuracy_score(y_test, vote_preds) * 100, 2)

            meta_model, stack_probs = stacking_meta(models, X_train, y_train, X_test)
            stack_preds = (stack_probs > 0.5).astype(int)
            acc_stack = round(accuracy_score(y_test, stack_preds) * 100, 2)

            st.markdown("### ğŸ“Š Káº¿t quáº£ Tá»•ng há»£p:")
            df = pd.DataFrame.from_dict(results, orient="index", columns=["Accuracy (%)"])
            st.table(df)
            st.markdown(f"**ğŸ§® Weighted Voting Accuracy:** {acc_vote}%")
            st.markdown(f"**ğŸ§  Stacking Meta-Learning Accuracy:** {acc_stack}%")
            best_model = max(results, key=results.get)
            st.markdown(f"ğŸ† **MÃ´ hÃ¬nh Ä‘Æ¡n tá»‘t nháº¥t:** {best_model} ({results[best_model]}%)")

st.info("ğŸ’¡ Tip: Báº¡n chá»‰ cáº§n huáº¥n luyá»‡n 1 láº§n. Sau Ä‘Ã³ cÃ³ thá»ƒ cháº¡y nhiá»u láº§n dá»± Ä‘oÃ¡n mÃ  khÃ´ng cáº§n huáº¥n luyá»‡n láº¡i.")

if __name__ == "__main__":
    st.write("")

import streamlit as st
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import SMOTE
from scipy.stats import entropy, zscore, skew, kurtosis
import matplotlib.pyplot as plt
import hashlib

# ------------------------------
# 1. HÃ m loáº¡i bá» hoáº·c thay tháº¿ outliers
# ------------------------------
@st.cache_data
def handle_outliers(window_data):
    """Xá»­ lÃ½ outliers báº±ng Z-score vÃ  thay tháº¿ báº±ng trung vá»‹."""
    arr = np.array(window_data, dtype=float)
    if len(arr) < 2:
        return arr.tolist()
    
    z_scores = np.abs(zscore(arr, ddof=1))
    median_val = np.median(arr)
    arr[z_scores > 3] = median_val  # thay tháº¿ giÃ¡ trá»‹ siÃªu nhiá»…u
    return arr.tolist()

# ------------------------------
# HÃ m tÃ­nh micro-patterns má»›i
# ------------------------------
def calculate_streaks(binary_seq):
    if not binary_seq:
        return 0
    current_streak = 1
    max_streak = 1
    for i in range(1, len(binary_seq)):
        if binary_seq[i] == binary_seq[i-1]:
            current_streak += 1
            max_streak = max(max_streak, current_streak)
        else:
            current_streak = 1
    return max_streak

def calculate_alternations(binary_seq):
    if len(binary_seq) < 2:
        return 0
    alternations = sum(1 for i in range(1, len(binary_seq)) if binary_seq[i] != binary_seq[i-1])
    return alternations / (len(binary_seq) - 1)

def calculate_autocorrelation(binary_seq, lag=1):
    if len(binary_seq) < lag + 1:
        return 0
    mean = np.mean(binary_seq)
    var = np.var(binary_seq)
    if var == 0:
        return 0
    ac = sum((binary_seq[i] - mean) * (binary_seq[i+lag] - mean) for i in range(len(binary_seq)-lag)) / (var * len(binary_seq))
    return ac

# ------------------------------
# HÃ m tÃ­nh bias áº©n
# ------------------------------
def calculate_bias_metrics(binary_seq):
    if len(binary_seq) < 2:
        return 0, 0, 0
    var = np.var(binary_seq)
    sk = skew(binary_seq)
    kur = kurtosis(binary_seq)
    return var, sk, kur

# ------------------------------
# 2. HÃ m táº¡o Ä‘áº·c trÆ°ng nÃ¢ng cao (ThÃªm micro-patterns + bias + feature selection)
# ------------------------------
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def create_advanced_features(history, window=5):
    encode = {"TÃ i": 1, "Xá»‰u": 0}
    history_num = [encode[r] for r in history]

    X, y = [], []
    for i in range(window, len(history_num)):
        basic_feats = history_num[i-window:i]
        basic_feats_clean = handle_outliers(basic_feats)

        counts = np.bincount(basic_feats_clean, minlength=2)
        probabilities = counts / counts.sum()
        entropy_val = entropy(probabilities, base=2)

        momentum = np.mean(np.diff(basic_feats_clean[-3:])) if len(basic_feats_clean) >= 2 else 0
        streaks = calculate_streaks(basic_feats_clean)
        alternations = calculate_alternations(basic_feats_clean)
        autocorr = calculate_autocorrelation(basic_feats_clean)
        var, sk, kur = calculate_bias_metrics(basic_feats_clean)

        features = basic_feats_clean + [entropy_val, momentum, streaks, alternations, autocorr, var, sk, kur]
        X.append(features)
        y.append(history_num[i])

    X = np.array(X)
    y = np.array(y)

    # Feature selection
    if len(X) > 0:
        selector = SelectKBest(f_classif, k=min(10, X.shape[1]))  # Giá»¯ tá»‘i Ä‘a 10 features tá»‘t nháº¥t
        X = selector.fit_transform(X, y)

    return X, y

# ------------------------------
# 3. PhÃ¢n tÃ­ch Ä‘á»™ ngáº«u nhiÃªn (ThÃªm micro-patterns + bias)
# ------------------------------
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def analyze_randomness_window(history, window=5):
    if len(history) < window:
        return "ğŸ”´ ChÆ°a Ä‘á»§ dá»¯ liá»‡u."
    encode = {"TÃ i": 1, "Xá»‰u": 0}
    last_window = [encode[r] for r in history[-window:]]
    last_window = handle_outliers(last_window)
    counts = np.bincount(last_window, minlength=2)
    probabilities = counts / counts.sum()
    ent_val = entropy(probabilities, base=2)
    
    streaks = calculate_streaks(last_window)
    alternations = calculate_alternations(last_window)
    autocorr = calculate_autocorrelation(last_window)
    var, sk, kur = calculate_bias_metrics(last_window)
    
    base_status = f"ğŸ”´ Entropy: {ent_val:.2f}. "
    if ent_val > 0.95:
        base_status += "Cá»±c ká»³ ngáº«u nhiÃªn! "
    elif ent_val > 0.85:
        base_status += "KhÃ¡ ngáº«u nhiÃªn. "
    elif ent_val > 0.70:
        base_status += "CÃ³ má»™t sá»‘ pattern. "
    else:
        base_status += "Pattern rÃµ rÃ ng. "
    
    micro_status = f"ğŸ” Micro-patterns: Max Streak={streaks}, Alternations={alternations:.2f}, Autocorr={autocorr:.2f}. "
    bias_status = f"ğŸ“Š Bias: Var={var:.2f}, Skew={sk:.2f}, Kurt={kur:.2f}."
    
    return base_status + micro_status + bias_status

# ------------------------------
# 4. Dá»± Ä‘oÃ¡n vÃ¡n tiáº¿p theo (Meta-Ensemble vá»›i weights Ä‘á»™ng + adaptive)
# ------------------------------
def predict_next_ensemble(models, weights, history, window=5, confidence_threshold=0.65):  # NÃ¢ng threshold lÃªn 0.65 cho small data
    encode = {"TÃ i": 1, "Xá»‰u": 0}
    if len(history) < window or not models:
        return "ChÆ°a Ä‘á»§ dá»¯ liá»‡u", 0.5, "ChÆ°a Ä‘á»§", np.nan

    last_window = [encode[r] for r in history[-window:]]
    last_window = handle_outliers(last_window)

    counts = np.bincount(last_window, minlength=2)
    probabilities = counts / counts.sum()
    entropy_val = entropy(probabilities, base=2)
    momentum = np.mean(np.diff(last_window[-3:])) if len(last_window) >= 2 else 0
    streaks = calculate_streaks(last_window)
    alternations = calculate_alternations(last_window)
    autocorr = calculate_autocorrelation(last_window)
    var, sk, kur = calculate_bias_metrics(last_window)

    final_feats = last_window + [entropy_val, momentum, streaks, alternations, autocorr, var, sk, kur]

    # Adaptive strategy
    adaptive_threshold = confidence_threshold
    if entropy_val > 0.85:
        adaptive_threshold = 0.70  # Cautious hÆ¡n náº¿u nhiá»…u cao
    if kur > 0:  # Peaked, cÃ³ pattern máº¡nh -> cautious hÆ¡n
        adaptive_threshold = 0.70
    if entropy_val > 0.85 and abs(sk) < 0.1:  # SiÃªu nhiá»…u, skew tháº¥p -> fallback majority
        majority = "TÃ i" if sum(last_window) > window / 2 else "Xá»‰u"
        return majority, 0.5, "Fallback do nhiá»…u cao âš ï¸", entropy_val
    if abs(autocorr) > 0.2:  # Pattern lagged máº¡nh -> cÃ³ thá»ƒ Æ°u tiÃªn LR/RF
        pass  # TODO: Æ¯u tiÃªn weights náº¿u cáº§n

    probs = []
    for model in models:
        try:
            prob = model.predict_proba([final_feats])[0][1]
        except:
            prob = 0.5
        probs.append(prob)
    probs = np.array(probs)
    final_prob_tai = np.clip(np.dot(weights, probs), 0.0, 1.0)

    if final_prob_tai > 0.5:
        pred, prob = "TÃ i", final_prob_tai
    else:
        pred, prob = "Xá»‰u", 1 - final_prob_tai

    confidence_status = "ÄÃ¡ng tin cáº­y âœ…"
    if prob < adaptive_threshold:
        confidence_status = f"Tháº¥p! (<{adaptive_threshold:.0%}) âš ï¸"
        pred = "KHÃ”NG Dá»° ÄOÃN"

    return pred, prob, confidence_status, entropy_val

# HÃ m tÃ­nh probs_list cho biá»ƒu Ä‘á»“, cached
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def compute_probs_list(history, window, _models, _weights):
    probs_list = []
    for i in range(window, len(history)):
        history_slice = history[:i]
        _, prob_tmp, _, _ = predict_next_ensemble(_models, _weights, history_slice, window, confidence_threshold=0.0)
        probs_list.append(prob_tmp)
    return probs_list

# ------------------------------
# 5. Streamlit App
# ------------------------------
st.set_page_config(page_title="ğŸ² AI Dá»± Ä‘oÃ¡n TÃ i Xá»‰u NÃ¢ng Cao", layout="wide")
st.title("ğŸ² AI Dá»± Ä‘oÃ¡n TÃ i Xá»‰u NÃ¢ng Cao")
st.markdown("**Meta-Ensemble (5 models, weights Ä‘á»™ng) | Micro-patterns + Bias + Adaptive Strategy + Tá»‘i Æ¯u cho Small Data**")

# Session state
if "history" not in st.session_state: 
    st.session_state.history = []
if "models" not in st.session_state: 
    st.session_state.models = None
if "weights" not in st.session_state: 
    st.session_state.weights = None
if "window" not in st.session_state: 
    st.session_state.window = 7
if "prev_hash" not in st.session_state:
    st.session_state.prev_hash = ""
window = st.session_state.window

# Giá»›i háº¡n history Ä‘á»ƒ trÃ¡nh cháº­m náº¿u quÃ¡ dÃ i
max_history = 1000
if len(st.session_state.history) > max_history:
    st.session_state.history = st.session_state.history[-max_history:]

# --- Nháº­p káº¿t quáº£ báº±ng 2 nÃºt ---
st.subheader("1. Nháº­p Káº¿t Quáº£ VÃ¡n ChÆ¡i")
col1, col2 = st.columns(2)
with col1:
    if st.button("ğŸ¯ TÃ i"):
        st.session_state.history.append("TÃ i")
with col2:
    if st.button("ğŸ¯ Xá»‰u"):
        st.session_state.history.append("Xá»‰u")

# TÃ­nh hash history Ä‘á»ƒ check thay Ä‘á»•i
def hash_history(hist):
    return hashlib.sha256(str(hist).encode()).hexdigest()

current_hash = hash_history(st.session_state.history)

# --- PhÃ¢n tÃ­ch lá»‹ch sá»­ ---
st.subheader("2. PhÃ¢n TÃ­ch Lá»‹ch Sá»­")
if st.session_state.history:
    st.write("Lá»‹ch sá»­ káº¿t quáº£ (má»›i nháº¥t cuá»‘i):", st.session_state.history)
    st.markdown(analyze_randomness_window(st.session_state.history, window))
    count_tai = st.session_state.history.count("TÃ i")
    count_xiu = st.session_state.history.count("Xá»‰u")
    total = len(st.session_state.history)
    st.write(f"TÃ i: {count_tai} ({count_tai/total:.2%}) | Xá»‰u: {count_xiu} ({count_xiu/total:.2%})")

# Chá»‰ huáº¥n luyá»‡n náº¿u history thay Ä‘á»•i vÃ  Ä‘á»§ data
if len(st.session_state.history) > window and (st.session_state.prev_hash != current_hash):
    X, y = create_advanced_features(st.session_state.history, window)
    
    # Xá»­ lÃ½ imbalance vá»›i SMOTE náº¿u cáº§n
    imbalance_ratio = abs(count_tai / total - 0.5)
    if imbalance_ratio > 0.1 and len(X) > 0:  # Imbalanced
        smote = SMOTE(random_state=42)
        X, y = smote.fit_resample(X, y)
    
    cv = StratifiedKFold(n_splits=5 if len(X) > 50 else 3)  # Äiá»u chá»‰nh folds cho small data
    
    # Tune vÃ  fit models vá»›i GridSearchCV
    # LR
    param_grid_lr = {'C': [0.1, 0.5, 1, 10]}
    grid_lr = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42), param_grid_lr, cv=cv)
    grid_lr.fit(X, y)
    model_lr = grid_lr.best_estimator_
    
    # RF
    param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [3, 5]}
    grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=cv)
    grid_rf.fit(X, y)
    model_rf = grid_rf.best_estimator_
    
    # XGB
    param_grid_xgb = {'n_estimators': [50, 100], 'max_depth': [3, 5]}
    grid_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid_xgb, cv=cv)
    grid_xgb.fit(X, y)
    model_xgb = grid_xgb.best_estimator_
    
    # SVM (Ã­t params)
    model_svm = SVC(probability=True, kernel='rbf', random_state=42).fit(X, y)
    
    # MLP
    param_grid_mlp = {'hidden_layer_sizes': [(20,), (50,)], 'max_iter': [200, 500]}
    grid_mlp = GridSearchCV(MLPClassifier(random_state=42), param_grid_mlp, cv=cv)
    grid_mlp.fit(X, y)
    model_mlp = grid_mlp.best_estimator_
    
    st.session_state.models = [model_lr, model_rf, model_xgb, model_svm, model_mlp]
    
    # TÃ­nh weights dá»±a trÃªn CV score (best_score_)
    accs = [grid_lr.best_score_, grid_rf.best_score_, grid_xgb.best_score_, accuracy_score(y, model_svm.predict(X)), grid_mlp.best_score_]
    st.session_state.weights = np.array(accs) / sum(accs) if sum(accs) > 0 else np.ones(len(accs)) / len(accs)
    st.info(f"âœ… ÄÃ£ huáº¥n luyá»‡n 5 models vá»›i {X.shape[0]} máº«u, {X.shape[1]} Ä‘áº·c trÆ°ng. Weights: {st.session_state.weights}")
    
    # ÄÃ¡nh giÃ¡ full
    y_pred_ensemble = np.argmax(np.array([model.predict_proba(X) for model in st.session_state.models]).mean(axis=0), axis=1)
    cm = confusion_matrix(y, y_pred_ensemble)
    pr = precision_recall_fscore_support(y, y_pred_ensemble, average='binary')
    st.write("Confusion Matrix:", cm)
    st.write(f"Precision: {pr[0]:.2f}, Recall: {pr[1]:.2f}, F1: {pr[2]:.2f}")

    st.session_state.prev_hash = current_hash

# Náº¿u Ä‘Ã£ cÃ³ models, hiá»ƒn thá»‹ dá»± Ä‘oÃ¡n
if st.session_state.models is not None and len(st.session_state.history) > window:
    # --- Dá»± Ä‘oÃ¡n vÃ¡n tiáº¿p theo ---
    st.subheader("3. Dá»± ÄoÃ¡n VÃ¡n Tiáº¿p Theo")
    pred, prob, conf_status, entropy_val = predict_next_ensemble(st.session_state.models, st.session_state.weights, st.session_state.history, window)
    st.markdown(f"**Dá»± Ä‘oÃ¡n (Meta-Ensemble):** **{pred}** | **Äá»™ tin cáº­y:** {prob:.2%} | Tráº¡ng thÃ¡i: {conf_status}")
    if pred == "KHÃ”NG Dá»° ÄOÃN":
        st.warning("âš ï¸ Äá»™ tin cáº­y tháº¥p. NÃªn cÃ¢n nháº¯c bá» qua vÃ¡n nÃ y.")

    # --- ÄÃ¡nh giÃ¡ vÃ¡n trÆ°á»›c ---
    if len(st.session_state.history) > window + 1:
        st.subheader("4. ÄÃ¡nh GiÃ¡ VÃ¡n TrÆ°á»›c")
        pred_prev, prob_prev, _, _ = predict_next_ensemble(st.session_state.models, st.session_state.weights, st.session_state.history[:-1], window, confidence_threshold=0.0)
        last_real = st.session_state.history[-1]
        lesson = "Tháº¯ng âœ…" if last_real == pred_prev else "Thua âŒ"
        st.markdown(f"**Káº¿t quáº£ vÃ¡n trÆ°á»›c:** {last_real} | **Dá»± Ä‘oÃ¡n:** {pred_prev} ({prob_prev:.2%}) | **BÃ i há»c:** {lesson}")

        # --- Biá»ƒu Ä‘á»“ xÃ¡c suáº¥t ---
        st.subheader("5. Biá»ƒu Äá»“ XÃ¡c Suáº¥t Dá»± ÄoÃ¡n")
        probs_list = compute_probs_list(st.session_state.history, window, st.session_state.models, st.session_state.weights)
        rounds = list(range(window+1, len(st.session_state.history)+1))
        fig, ax = plt.subplots(figsize=(10,5))
        ax.plot(rounds, probs_list, color='purple', marker='o', markersize=3, label="XÃ¡c suáº¥t Tin cáº­y")
        ax.axhline(0.65, color='red', linestyle='--', label="NgÆ°á»¡ng Tin Cáº­y 65%")
        ax.axhline(0.5, color='gray', linestyle='-', label="NgÆ°á»¡ng CÆ¡ báº£n 50%")
        ax.set_xlabel("VÃ¡n chÆ¡i")
        ax.set_ylabel("Äá»™ Tin Cáº­y Dá»± ÄoÃ¡n")
        ax.set_title("Äá»™ Tin Cáº­y Dá»± ÄoÃ¡n qua cÃ¡c VÃ¡n")
        ax.legend()
        st.pyplot(fig)
else:
    st.info(f"Cáº§n Ã­t nháº¥t {window+1} káº¿t quáº£ Ä‘á»ƒ huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n.")

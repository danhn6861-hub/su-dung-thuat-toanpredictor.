```python
import streamlit as st
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import SMOTE
from scipy.stats import entropy, zscore, skew, kurtosis
import matplotlib.pyplot as plt
import hashlib

# ------------------------------
# 1. H√†m lo·∫°i b·ªè ho·∫∑c thay th·∫ø outliers
# ------------------------------
@st.cache_data
def handle_outliers(window_data):
    """X·ª≠ l√Ω outliers b·∫±ng Z-score v√† thay th·∫ø b·∫±ng trung v·ªã."""
    arr = np.array(window_data, dtype=float)
    if len(arr) < 2:
        return arr.tolist()
    
    z_scores = np.abs(zscore(arr, ddof=1))
    median_val = np.median(arr)
    arr[z_scores > 3] = median_val
    return arr.tolist()

# ------------------------------
# H√†m t√≠nh micro-patterns
# ------------------------------
def calculate_streaks(binary_seq):
    if not binary_seq:
        return 0
    current_streak = 1
    max_streak = 1
    for i in range(1, len(binary_seq)):
        if binary_seq[i] == binary_seq[i-1]:
            current_streak += 1
            max_streak = max(max_streak, current_streak)
        else:
            current_streak = 1
    return max_streak

def calculate_alternations(binary_seq):
    if len(binary_seq) < 2:
        return 0
    alternations = sum(1 for i in range(1, len(binary_seq)) if binary_seq[i] != binary_seq[i-1])
    return alternations / (len(binary_seq) - 1)

def calculate_autocorrelation(binary_seq, lag=1):
    if len(binary_seq) < lag + 1:
        return 0
    mean = np.mean(binary_seq)
    var = np.var(binary_seq)
    if var == 0:
        return 0
    ac = sum((binary_seq[i] - mean) * (binary_seq[i+lag] - mean) for i in range(len(binary_seq)-lag)) / (var * len(binary_seq))
    return ac

# ------------------------------
# H√†m t√≠nh bias ·∫©n
# ------------------------------
def calculate_bias_metrics(binary_seq):
    if len(binary_seq) < 2:
        return 0, 0, 0
    var = np.var(binary_seq)
    sk = skew(binary_seq)
    kur = kurtosis(binary_seq)
    return var, sk, kur

# ------------------------------
# 2. H√†m t·∫°o ƒë·∫∑c tr∆∞ng n√¢ng cao
# ------------------------------
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def create_advanced_features(history, window=5):
    encode = {"T√†i": 1, "X·ªâu": 0}
    history_num = [encode[r] for r in history]

    X, y = [], []
    for i in range(window, len(history_num)):
        basic_feats = history_num[i-window:i]
        basic_feats_clean = handle_outliers(basic_feats)

        counts = np.bincount(basic_feats_clean, minlength=2)
        probabilities = counts / counts.sum()
        entropy_val = entropy(probabilities, base=2)

        momentum = np.mean(np.diff(basic_feats_clean[-3:])) if len(basic_feats_clean) >= 2 else 0
        streaks = calculate_streaks(basic_feats_clean)
        alternations = calculate_alternations(basic_feats_clean)
        autocorr = calculate_autocorrelation(basic_feats_clean)
        var, sk, kur = calculate_bias_metrics(basic_feats_clean)

        features = basic_feats_clean + [entropy_val, momentum, streaks, alternations, autocorr, var, sk, kur]
        X.append(features)
        y.append(history_num[i])

    X = np.array(X)
    y = np.array(y)

    # Feature selection
    if len(X) > 0:
        selector = SelectKBest(f_classif, k=min(10, X.shape[1]))
        X = selector.fit_transform(X, y)

    return X, y

# ------------------------------
# 3. Ph√¢n t√≠ch ƒë·ªô ng·∫´u nhi√™n
# ------------------------------
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def analyze_randomness_window(history, window=5):
    if len(history) < window:
        return "üî¥ Ch∆∞a ƒë·ªß d·ªØ li·ªáu."
    encode = {"T√†i": 1, "X·ªâu": 0}
    last_window = [encode[r] for r in history[-window:]]
    last_window = handle_outliers(last_window)
    counts = np.bincount(last_window, minlength=2)
    probabilities = counts / counts.sum()
    ent_val = entropy(probabilities, base=2)
    
    streaks = calculate_streaks(last_window)
    alternations = calculate_alternations(last_window)
    autocorr = calculate_autocorrelation(last_window)
    var, sk, kur = calculate_bias_metrics(last_window)
    
    base_status = f"üî¥ Entropy: {ent_val:.2f}. "
    if ent_val > 0.95:
        base_status += "C·ª±c k·ª≥ ng·∫´u nhi√™n! "
    elif ent_val > 0.85:
        base_status += "Kh√° ng·∫´u nhi√™n. "
    elif ent_val > 0.70:
        base_status += "C√≥ m·ªôt s·ªë pattern. "
    else:
        base_status += "Pattern r√µ r√†ng. "
    
    micro_status = f"üîç Micro-patterns: Max Streak={streaks}, Alternations={alternations:.2f}, Autocorr={autocorr:.2f}. "
    bias_status = f"üìä Bias: Var={var:.2f}, Skew={sk:.2f}, Kurt={kur:.2f}."
    
    return base_status + micro_status + bias_status

# ------------------------------
# 4. D·ª± ƒëo√°n v√°n ti·∫øp theo
# ------------------------------
def predict_next_ensemble(models, weights, history, window=5, confidence_threshold=0.65):
    encode = {"T√†i": 1, "X·ªâu": 0}
    if len(history) < window or not models:
        return "Ch∆∞a ƒë·ªß d·ªØ li·ªáu", 0.5, "Ch∆∞a ƒë·ªß", np.nan

    last_window = [encode[r] for r in history[-window:]]
    last_window = handle_outliers(last_window)

    counts = np.bincount(last_window, minlength=2)
    probabilities = counts / counts.sum()
    entropy_val = entropy(probabilities, base=2)
    momentum = np.mean(np.diff(last_window[-3:])) if len(last_window) >= 2 else 0
    streaks = calculate_streaks(last_window)
    alternations = calculate_alternations(last_window)
    autocorr = calculate_autocorrelation(last_window)
    var, sk, kur = calculate_bias_metrics(last_window)

    final_feats = last_window + [entropy_val, momentum, streaks, alternations, autocorr, var, sk, kur]

    # Adaptive strategy
    adaptive_threshold = confidence_threshold
    if entropy_val > 0.85:
        adaptive_threshold = 0.70
    if kur > 0:
        adaptive_threshold = 0.70
    if entropy_val > 0.85 and abs(sk) < 0.1:
        majority = "T√†i" if sum(last_window) > window / 2 else "X·ªâu"
        return majority, 0.5, "Fallback do nhi·ªÖu cao ‚ö†Ô∏è", entropy_val

    probs = []
    for model in models:
        try:
            prob = model.predict_proba([final_feats])[0][1]
        except:
            prob = 0.5
        probs.append(prob)
    probs = np.array(probs)
    final_prob_tai = np.clip(np.dot(weights, probs), 0.0, 1.0)

    if final_prob_tai > 0.5:
        pred, prob = "T√†i", final_prob_tai
    else:
        pred, prob = "X·ªâu", 1 - final_prob_tai

    confidence_status = "ƒê√°ng tin c·∫≠y ‚úÖ"
    if prob < adaptive_threshold:
        confidence_status = f"Th·∫•p! (<{adaptive_threshold:.0%}) ‚ö†Ô∏è"
        pred = "KH√îNG D·ª∞ ƒêO√ÅN"

    return pred, prob, confidence_status, entropy_val

# ------------------------------
# H√†m t√≠nh probs_list cho bi·ªÉu ƒë·ªì
# ------------------------------
@st.cache_data(hash_funcs={list: lambda x: hashlib.sha256(str(x).encode()).hexdigest()})
def compute_probs_list(history, window, _models, _weights):
    probs_list = []
    for i in range(window, len(history)):
        history_slice = history[:i]
        _, prob_tmp, _, _ = predict_next_ensemble(_models, _weights, history_slice, window, confidence_threshold=0.0)
        probs_list.append(prob_tmp)
    return probs_list

# ------------------------------
# 5. Streamlit App
# ------------------------------
st.set_page_config(page_title="üé≤ AI D·ª± ƒëo√°n T√†i X·ªâu N√¢ng Cao", layout="wide")
st.title("üé≤ AI D·ª± ƒëo√°n T√†i X·ªâu N√¢ng Cao")
st.markdown("**Meta-Ensemble (5 models, weights ƒë·ªông) | Micro-patterns + Bias + Adaptive Strategy + T·ªëi ∆Øu cho Small Data**")

# Session state
if "history" not in st.session_state: 
    st.session_state.history = []
if "models" not in st.session_state: 
    st.session_state.models = None
if "weights" not in st.session_state: 
    st.session_state.weights = None
if "window" not in st.session_state: 
    st.session_state.window = 7
if "prev_hash" not in st.session_state:
    st.session_state.prev_hash = ""
if "force_train" not in st.session_state:
    st.session_state.force_train = False
window = st.session_state.window

# Gi·ªõi h·∫°n history
max_history = 1000
if len(st.session_state.history) > max_history:
    st.session_state.history = st.session_state.history[-max_history:]

# --- Nh·∫≠p k·∫øt qu·∫£ v√† qu·∫£n l√Ω d·ªØ li·ªáu ---
st.subheader("1. Nh·∫≠p K·∫øt Qu·∫£ V√°n Ch∆°i v√† Qu·∫£n L√Ω D·ªØ Li·ªáu")
col1, col2, col3, col4 = st.columns(4)
with col1:
    if st.button("üéØ T√†i"):
        st.session_state.history.append("T√†i")
with col2:
    if st.button("üéØ X·ªâu"):
        st.session_state.history.append("X·ªâu")
with col3:
    if st.button("üõ†Ô∏è Hu·∫•n Luy·ªán M√¥ H√¨nh"):
        st.session_state.force_train = True
with col4:
    if st.button("üóëÔ∏è X√≥a To√†n B·ªô D·ªØ Li·ªáu"):
        st.session_state.history = []
        st.session_state.models = None
        st.session_state.weights = None
        st.session_state.prev_hash = ""
        st.session_state.force_train = False
        st.success("ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu.")

# T√≠nh hash history
def hash_history(hist):
    return hashlib.sha256(str(hist).encode()).hexdigest()

current_hash = hash_history(st.session_state.history)

# --- Ph√¢n t√≠ch l·ªãch s·ª≠ ---
st.subheader("2. Ph√¢n T√≠ch L·ªãch S·ª≠")
if st.session_state.history:
    st.write("L·ªãch s·ª≠ k·∫øt qu·∫£ (m·ªõi nh·∫•t cu·ªëi):", st.session_state.history)
    st.markdown(analyze_randomness_window(st.session_state.history, window))
    count_tai = st.session_state.history.count("T√†i")
    count_xiu = st.session_state.history.count("X·ªâu")
    total = len(st.session_state.history)
    st.write(f"T√†i: {count_tai} ({count_tai/total:.2%}) | X·ªâu: {count_xiu} ({count_xiu/total:.2%})")
else:
    st.info("Ch∆∞a c√≥ d·ªØ li·ªáu l·ªãch s·ª≠.")

# --- Hu·∫•n luy·ªán m√¥ h√¨nh ---
if len(st.session_state.history) > window and (st.session_state.prev_hash != current_hash or st.session_state.force_train):
    X, y = create_advanced_features(st.session_state.history, window)
    
    # Validate data
    if len(X) < 10 or len(np.unique(y)) < 2:
        st.error("D·ªØ li·ªáu qu√° nh·ªè ho·∫∑c ch·ªâ c√≥ m·ªôt class. C·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán.")
    else:
        # Check class distribution
        class_counts = np.bincount(y, minlength=2)
        min_class_count = min(class_counts)
        n_splits = min(5, len(X), min_class_count) if min_class_count > 0 else 2
        
        # X·ª≠ l√Ω imbalance v·ªõi SMOTE n·∫øu c·∫ßn
        imbalance_ratio = abs(count_tai / total - 0.5)
        if imbalance_ratio > 0.1 and len(X) > 10 and min_class_count > 5:
            try:
                smote = SMOTE(random_state=42, k_neighbors=min(3, min_class_count-1))
                X, y = smote.fit_resample(X, y)
                st.info("ƒê√£ √°p d·ª•ng SMOTE ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu.")
            except:
                st.warning("Kh√¥ng th·ªÉ √°p d·ª•ng SMOTE do d·ªØ li·ªáu kh√¥ng ƒë·ªß. Ti·∫øp t·ª•c v·ªõi d·ªØ li·ªáu g·ªëc.")
        
        # Cross-validation
        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        # Tune v√† fit models
        try:
            # LR
            param_grid_lr = {'C': [0.1, 0.5, 1, 10]}
            grid_lr = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42), param_grid_lr, cv=cv)
            grid_lr.fit(X, y)
            model_lr = grid_lr.best_estimator_
            
            # RF
            param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [3, 5]}
            grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=cv)
            grid_rf.fit(X, y)
            model_rf = grid_rf.best_estimator_
            
            # XGB
            param_grid_xgb = {'n_estimators': [50, 100], 'max_depth': [3, 5]}
            grid_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid_xgb, cv=cv)
            grid_xgb.fit(X, y)
            model_xgb = grid_xgb.best_estimator_
            
            # SVM
            model_svm = SVC(probability=True, kernel='rbf', random_state=42).fit(X, y)
            
            # MLP
            param_grid_mlp = {'hidden_layer_sizes': [(20,), (50,)], 'max_iter': [200, 500]}
            grid_mlp = GridSearchCV(MLPClassifier(random_state=42), param_grid_mlp, cv=cv)
            grid_mlp.fit(X, y)
            model_mlp = grid_mlp.best_estimator_
            
            st.session_state.models = [model_lr, model_rf, model_xgb, model_svm, model_mlp]
            
            # T√≠nh weights
            accs = [grid_lr.best_score_, grid_rf.best_score_, grid_xgb.best_score_, accuracy_score(y, model_svm.predict(X)), grid_mlp.best_score_]
            st.session_state.weights = np.array(accs) / sum(accs) if sum(accs) > 0 else np.ones(len(accs)) / len(accs)
            st.info(f"‚úÖ ƒê√£ hu·∫•n luy·ªán 5 models v·ªõi {X.shape[0]} m·∫´u, {X.shape[1]} ƒë·∫∑c tr∆∞ng. Weights: {st.session_state.weights}")
            
            # ƒê√°nh gi√° full
            y_pred_ensemble = np.argmax(np.array([model.predict_proba(X) for model in st.session_state.models]).mean(axis=0), axis=1)
            cm = confusion_matrix(y, y_pred_ensemble)
            pr = precision_recall_fscore_support(y, y_pred_ensemble, average='binary')
            st.write("Confusion Matrix:", cm)
            st.write(f"Precision: {pr[0]:.2f}, Recall: {pr[1]:.2f}, F1: {pr[2]:.2f}")
            
            st.session_state.prev_hash = current_hash
            st.session_state.force_train = False
        except Exception as e:
            st.error(f"L·ªói hu·∫•n luy·ªán: {str(e)}. Th·ª≠ th√™m d·ªØ li·ªáu ho·∫∑c ki·ªÉm tra l·∫°i l·ªãch s·ª≠.")

# --- D·ª± ƒëo√°n v√† ƒë√°nh gi√° ---
if st.session_state.models is not None and len(st.session_state.history) > window:
    # D·ª± ƒëo√°n v√°n ti·∫øp theo
    st.subheader("3. D·ª± ƒêo√°n V√°n Ti·∫øp Theo")
    pred, prob, conf_status, entropy_val = predict_next_ensemble(st.session_state.models, st.session_state.weights, st.session_state.history, window)
    st.markdown(f"**D·ª± ƒëo√°n (Meta-Ensemble):** **{pred}** | **ƒê·ªô tin c·∫≠y:** {prob:.2%} | Tr·∫°ng th√°i: {conf_status}")
    if pred == "KH√îNG D·ª∞ ƒêO√ÅN":
        st.warning("‚ö†Ô∏è ƒê·ªô tin c·∫≠y th·∫•p. N√™n c√¢n nh·∫Øc b·ªè qua v√°n n√†y.")

    # ƒê√°nh gi√° v√°n tr∆∞·ªõc
    if len(st.session_state.history) > window + 1:
        st.subheader("4. ƒê√°nh Gi√° V√°n Tr∆∞·ªõc")
        pred_prev, prob_prev, _, _ = predict_next_ensemble(st.session_state.models, st.session_state.weights, st.session_state.history[:-1], window, confidence_threshold=0.0)
        last_real = st.session_state.history[-1]
        lesson = "Th·∫Øng ‚úÖ" if last_real == pred_prev else "Thua ‚ùå"
        st.markdown(f"**K·∫øt qu·∫£ v√°n tr∆∞·ªõc:** {last_real} | **D·ª± ƒëo√°n:** {pred_prev} ({prob_prev:.2%}) | **B√†i h·ªçc:** {lesson}")

        # Bi·ªÉu ƒë·ªì x√°c su·∫•t
        st.subheader("5. Bi·ªÉu ƒê·ªì X√°c Su·∫•t D·ª± ƒêo√°n")
        probs_list = compute_probs_list(st.session_state.history, window, st.session_state.models, st.session_state.weights)
        rounds = list(range(window+1, len(st.session_state.history)+1))
        fig, ax = plt.subplots(figsize=(10,5))
        ax.plot(rounds, probs_list, color='purple', marker='o', markersize=3, label="X√°c su·∫•t Tin c·∫≠y")
        ax.axhline(0.65, color='red', linestyle='--', label="Ng∆∞·ª°ng Tin C·∫≠y 65%")
        ax.axhline(0.5, color='gray', linestyle='-', label="Ng∆∞·ª°ng C∆° B·∫£n 50%")
        ax.set_xlabel("V√°n ch∆°i")
        ax.set_ylabel("ƒê·ªô Tin C·∫≠y D·ª± ƒêo√°n")
        ax.set_title("ƒê·ªô Tin C·∫≠y D·ª± ƒêo√°n qua c√°c V√°n")
        ax.legend()
        st.pyplot(fig)
else:
    st.info(f"C·∫ßn √≠t nh·∫•t {window+1} k·∫øt qu·∫£ ƒë·ªÉ hu·∫•n luy·ªán v√† d·ª± ƒëo√°n.")
```

### Instructions to Fix the Error
#### 1. **Update `app.py` on Streamlit Cloud**
- **Problem**: The current `app.py` file on your Streamlit Cloud repository (`/mount/src/tai-xiu-predictor./app.py`) contains invalid Markdown code block markers (````python`).
- **Solution**:
  1. Access your GitHub repository (or wherever your Streamlit Cloud app is hosted).
  2. Open the `app.py` file in the `tai-xiu-predictor` directory.
  3. Replace its contents with the corrected code above (without ````python` and closing `````).
  4. Save and commit the changes.
  5. Go to Streamlit Cloud, click "Manage app" (bottom right), and select "Reboot" to rebuild the app.
- **Verify**: Check the logs in "Manage app" > Logs to ensure no `SyntaxError`.

#### 2. **Ensure `requirements.txt` is Correct**
- The `requirements.txt` file from your previous request is correct and includes all dependencies:
  ```text
  streamlit>=1.38.0
  numpy>=1.26.4
  scikit-learn>=1.5.2
  xgboost>=2.1.1
  imblearn>=0.12.3
  scipy>=1.14.1
  matplotlib>=3.9.2
  ```
- **Action**:
  - Ensure `requirements.txt` is in the root of your repository (same directory as `app.py`).
  - If you previously had issues with `xgboost`, add a `packages.txt` file to handle system-level dependencies:
    ```text
    g++ build-essential
    ```
  - Commit and push both files to your repository, then reboot the app on Streamlit Cloud.

#### 3. **Test Locally First**
To avoid repeated deployment errors:
- Save the corrected `app.py` and `requirements.txt` locally.
- Create a virtual environment:
  ```bash
  python -m venv venv
  source venv/bin/activate  # Linux/Mac
  venv\Scripts\activate     # Windows
  ```
- Install dependencies:
  ```bash
  pip install -r requirements.txt
  ```
- Run the app:
  ```bash
  streamlit run app.py
  ```
- Test by adding a few "T√†i"/"X·ªâu" entries, clicking "Hu·∫•n Luy·ªán M√¥ H√¨nh," and verifying no errors.

#### 4. **Prevent Future Issues**
- **Avoid Markdown markers**: When copying code from forums, documentation, or responses, ensure you exclude ````python` or similar markers.
- **Check file before committing**: Open `app.py` in a text editor (e.g., VSCode) to confirm it starts with `import streamlit as st` and has no extra characters.
- **Use GitHub editor**: If editing on Streamlit Cloud, use the GitHub web editor to update `app.py` directly and verify syntax.

### Additional Notes
- **Previous `ValueError` Fix**: The code retains all fixes for the previous `ValueError` in `GridSearchCV` (dynamic `n_splits`, data validation, SMOTE handling), so it should handle your ~120-game dataset robustly.
- **New Buttons**:
  - **Train Button ("Hu·∫•n Luy·ªán M√¥ H√¨nh")**: Triggers training manually, reducing unnecessary retraining.
  - **Clear Data Button ("X√≥a To√†n B·ªô D·ªØ Li·ªáu")**: Resets all state, confirmed with a success message.
- **Performance**: The app uses `@st.cache_data` and reduced model complexity (`n_estimators=50/100`, `max_depth=3/5`) to ensure fast execution, even on Streamlit Cloud.
- **Testing with Data**: If you provide a 120-game history (e.g., a list of "T√†i"/"X·ªâu"), I can simulate predictions to verify accuracy and check for other potential issues.

### Next Steps
1. Update `app.py` with the corrected code.
2. Ensure `requirements.txt` (and `packages.txt` if needed) is in your repository.
3. Reboot the app on Streamlit Cloud and check logs for errors.
4. Test locally to confirm functionality.
5. If the error persists or you encounter new issues, share the full log from Streamlit Cloud or a sample history for further debugging.

Let me know if you need help with deployment or testing!
